---
title: "Homework assignment"
subtitle: "Data Science 1: Machine Learning Concepts"
author: "Viktória Mészáros"
date: "22/02/2021"
output:
  prettydoc::html_pretty:
    theme: cayman
    highlight: github
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r, include=FALSE}
# Clean environment
rm(list = ls())

# load packages
library(tidyverse)
library(datasets)
library(MASS)
library(ISLR)
library(caret)
library(skimr)
library(GGally)
library(NbClust)
library(factoextra)
library(kableExtra)
library(robustHD)
library(DescTools)

```

# 1. Supervised learning with penalized models and PCA
In this exercise I am analyze a dataset about property values in Manhattan. This dataset is borrowed from the book [“R for Everyone”](https://www.jaredlander.com/r-for-everyone/data-in-r-for-everyone/) by Jared Lander. The goal will be to predict the logarithm of the property value: logTotalValue.
```{r, include=F}
# read in the data, and create logTotalValue variable
data <- readRDS(url('http://www.jaredlander.com/data/manhattan_Train.rds')) %>%
  mutate(logTotalValue = log(TotalValue)) 

# drop id as it is unique for each 
data$ID <- NULL

```

### a) Do a short exploration of data and find possible predictors of the target variable.
I started the exploration of the data by checking missing values. In the original data there were missing values only for the CommFAR variable (504) which is insignificant compared to the number of observations we have, so I deleted there from the data. There was a tricky variable namely ZoneDist where there were actually a lot of missing values but these were not NA but "Missing". I checked what percentage of the observation are missing for each ZoneDist variable. As more than 99.9% were Missing for ZoneDist3 and ZoneDist4 I decided the delete these. (These were close to zero variance features, so they would not improvee the models). 
```{r, echo=F, message=F, warning=F}
# Missing values 
count_missing_values <- function(data) {
  num_missing_values <- map_int(data, function(x) sum(is.na(x)))
  num_missing_values[num_missing_values > 0]
}
count_missing_values(data)

data <- data %>% drop_na()

# ZoneDist variables
sum(data$ZoneDist2 == "Missing")/nrow(data)*100 # 92.89%
sum(data$ZoneDist3 == "Missing")/nrow(data)*100 # 99.93%
sum(data$ZoneDist4 == "Missing")/nrow(data)*100 # 99.997%

data$ZoneDist3 <- NULL
data$ZoneDist4 <- NULL
```
For unique values I created a table to understand how many different values each feature has and if they are in the righ format (Factor, numeric, etc.). I found 3 variables which were numeric but actually they should have been factor (Council, PolicePrct, HealtArea), so I change the type of those. I also changed Yes-No values to 0-1s.
```{r, echo=F, message=F, warning=F}
# Unique values
uniq <- data.frame(columns = colnames(data))

for (i in 1:nrow(uniq)){
  uniq$unique_values[i] <- nrow(unique(data[,uniq$columns[i]]))
  uniq$class[i] <- class(data[,uniq$columns[i]][[1]])
}

uniq %>% 
  kbl() %>% 
  kable_classic(full_width = F, html_font = "Cambria")

# Council, PolicePrct and HealthArea should be factors
data <-
data %>% mutate(
  Council = factor(Council),
  PolicePrct = factor(PolicePrct),
  HealthArea = factor(HealthArea)
)

# IrregularLot, Landmark, HistoricDistrict and High will be 0-1 binary variables
data <-
  data %>% mutate(
    IrregularLot     = ifelse(IrregularLot == "Yes",1,0),
    Landmark         = ifelse(Landmark == "Yes",1,0),
    HistoricDistrict = ifelse(HistoricDistrict == "Yes" ,1,0),
    High             = ifelse(High == T,1,0))
```


```{r, include=F}
# I made a quick overview of the data
str(data)
skim(data)
summary(data)
```

#### Target variable
I already know I had to find the logToltaValue, but I wanted to have a look at the distribution and why we decided to take the log. It is really nicely showed from the data that the original value had a skewed distribution with a long right tail, so it made sense to make a logarithmic transformation as with that the distribution is more like normal.  
```{r, echo=F, message=F, warning=F, out.width="50%"}
## TotalValue
ggplot(data =  data, aes (x = TotalValue, y = ..density..)) +
  geom_histogram(color = "white", fill = "deepskyblue4",alpha = 0.8)+
  labs(x = "Total value", y = "Frequency") +
  theme_bw()

## LogTotlaValue
ggplot(data =  data, aes (x = log(TotalValue), y = ..density..)) +
  geom_histogram(color = "white", fill = "springgreen4",alpha = 0.8)+
  labs(x = "Log Total value", y = "Frequency") +
  theme_bw()

# delete TotalValue as we won't need it from now on
data$TotalValue <- NULL
```

#### Numeric variables
I looked at the distributions for all numberic variables. You can see that most of them is skewed witha a long right tail. I decided to do winzoring on my data. For all the columns that looked skewed I changed the 3 higest extreme values to the 4th highest value. I did not want to make a lot of changes, as the hihger values could have important meaning as well, so I decicded on this top 3 replacement as it is only excluding the highly extreme values that could alter the models a lot.
```{r, echo=F, message=F, warning=F, fig.align='center'}
data %>%
  keep(is.numeric) %>% 
  gather() %>% 
  ggplot(aes(value)) +
  facet_wrap(~key, scales = "free") +
  geom_histogram(color = "deepskyblue4")+
  theme_bw() 

```

```{r, include=F}
# Winzoring by hand - change the values on the 3 highest extreme values
for (i in 1:nrow(data)){
   data$LotArea[i] <- ifelse(data$LotArea[i] > head(Large(data$LotArea, k=4), 1), head(Large(data$LotArea, k=4), 1), data$LotArea[i])  
  data$BldgArea[i] <- ifelse(data$BldgArea[i] > head(Large(data$BldgArea, k=4), 1), head(Large(data$BldgArea, k=4), 1), data$BldgArea[i])
  data$ComArea[i] <- ifelse(data$ComArea[i] > head(Large(data$ComArea, k=4), 1), head(Large(data$ComArea, k=4), 1), data$ComArea[i])
  data$ResArea[i] <- ifelse(data$ResArea[i] > head(Large(data$ResArea, k=4), 1), head(Large(data$ResArea, k=4), 1), data$ResArea[i])
  data$OfficeArea[i] <- ifelse(data$OfficeArea[i] > head(Large(data$OfficeArea, k=4), 1), head(Large(data$OfficeArea, k=4), 1), data$OfficeArea[i])
    data$RetailArea[i] <- ifelse(data$RetailArea[i] > head(Large(data$RetailArea, k=4), 1), head(Large(data$RetailArea, k=4), 1), data$RetailArea[i])
    data$GarageArea[i] <- ifelse(data$GarageArea[i] > head(Large(data$GarageArea, k=4), 1), head(Large(data$GarageArea, k=4), 1), data$GarageArea[i])
    data$StrgeArea[i] <- ifelse(data$StrgeArea[i] > head(Large(data$StrgeArea, k=4), 1), head(Large(data$StrgeArea, k=4), 1), data$StrgeArea[i])
    data$FactryArea[i] <- ifelse(data$FactryArea[i] > head(Large(data$FactryArea, k=4), 1), head(Large(data$FactryArea, k=4), 1), data$FactryArea[i])
    data$OtherArea[i] <- ifelse(data$OtherArea[i] > head(Large(data$OtherArea, k=4), 1), head(Large(data$OtherArea, k=4), 1), data$OtherArea[i])
    data$NumBldgs[i] <- ifelse(data$NumBldgs[i] > head(Large(data$NumBldgs, k=4), 1), head(Large(data$NumBldgs, k=4), 1), data$NumBldgs[i])
    data$NumFloors[i] <- ifelse(data$NumFloors[i] > head(Large(data$NumFloors, k=4), 1), head(Large(data$NumFloors, k=4), 1), data$NumFloors[i])
    data$UnitsRes[i] <- ifelse(data$UnitsRes[i] > head(Large(data$UnitsRes, k=4), 1), head(Large(data$UnitsRes, k=4), 1), data$UnitsRes[i])
    data$UnitsTotal[i] <- ifelse(data$UnitsTotal[i] > head(Large(data$UnitsTotal, k=4), 1), head(Large(data$UnitsTotal, k=4), 1), data$UnitsTotal[i])
    data$LotFront[i] <- ifelse(data$LotFront[i] > head(Large(data$LotFront, k=4), 1), head(Large(data$LotFront, k=4), 1), data$LotFront[i])
    data$BldgDepth[i] <- ifelse(data$BldgDepth[i] > head(Large(data$BldgDepth, k=4), 1), head(Large(data$BldgDepth, k=4), 1), data$BldgDepth[i])
    data$BuiltFAR[i] <- ifelse(data$BuiltFAR[i] > head(Large(data$BuiltFAR, k=4), 1), head(Large(data$BuiltFAR, k=4), 1), data$BuiltFAR[i])
  
}
```

#### Correlation
I looked at correlations as well, first with a correlation matrix, from which I managed to get a deeper understanding of the interactions between variables, and more importantly o which features have the highest correlation with my Target variable

```{r, echo=F, message=F, warning=F, fig.align='center'}
## Correlations
ggcorr(data)
```

The most important features that correlates most with logTotalValue are BldgArea, NumFloors, LotFront, BuiltFARand High. For these I created the paired graphs to see how the logTotalValue depends on them.

```{r, echo=F, message=F, warning=F, fig.align='center'}
ggpairs(data, columns = c("logTotalValue", "BldgArea", "NumFloors", "LotFront", "BuiltFAR", "High"))
```

### b) Creating training and test set
Before building any models I separated the data into training and test sets. I put 30% of the observations totally randomly to a train test, while the remaining 70% became part of the test set. I will use the traning set tobuild all my models and only use the test set to evaluate them on "new data".  

```{r, message=F, warning=F}
set.seed(2021)

training_ratio <- 0.3

train_indices <- createDataPartition(
  y = data[["logTotalValue"]],
  times = 1,
  p = training_ratio,
  list = FALSE
) %>% as.vector()
data_train <- data[train_indices, ]
data_test <- data[-train_indices, ]
```

### c) Use a linear regression to predict logTotalValue
I created a basic linear regression model to predict the logTotalValue. This will serve as a benchmark model for later on, so I can check if other models can improve the performance of this one. 
I used 10-fold cross validation to assess predictive power and averaged the RMSE and R squared on the folds. This is also good as this way we get more robust results and the model is less sensitive to sample specific issues.

The R squared is 0.83 which means that our model is capturing 83% on the variation in the outcome variable, logTotalValue and the remaining 17% is left to other factors. The RMSE is 0.65 in the training set and 0.68 in the test set. This higher value is normal. It could mean that we overfit the test set, but in this case this is not the case. The values are really close. It is also important to mention that the train set is only 30% of the data and the test is 70% so there may be patterns that are not represented in the train set, but are in the test data. (I would probably try to use 70-80% od the data as training and look if this changes.)


```{r, echo=F, message=F, warning=F, cache=F}
train_control <- trainControl(method = "cv", number = 10)

set.seed(2021)
linear_model <- train(
  logTotalValue ~ .,
  data = data_train,
  method = "lm",
  preProcess = c("center", "scale"),
  trControl = train_control)

#summary(linear_model)

linear_RMSE <- linear_model$resample[,"RMSE"] %>% mean()
linear_R2 <- linear_model$resample[,"Rsquared"] %>%  mean()

data_test$linear_pred <- predict(linear_model, newdata = data_test)

linear_test_RMSE <- RMSE(data_test$linear_pred, data_test$logTotalValue)

linear_res <- cbind(linear_R2, linear_RMSE, linear_test_RMSE )

linear_res %>% 
  kbl() %>% 
  kable_classic(full_width = F, html_font = "Cambria")
```

### d.) Use penalized linear models for the same task
After I built the baseline simple linear model I used different penalized models. These put a penalty on the sum of regression coefficients either in absolute or squared terms. This will lead to finding less complex models and also solve the problem of correlating features.

* I used the **caret** package *

#### Ridge model
Ridge model puts a penalty on the sum of squared residuals. I checked several lambda values between 0.01 and 0.7 to find the best, which leads to the lowest cross validated RMSE. Lambda is the weight of the penalty term. The higher it is the simpler the model will get. In this case it was 0.11.

```{r, echo=F, message=F,warning=F, cache=TRUE, fig.align='center', out.width="60%"}
#######################
##    Ridge model    ##
#######################

# in caret I need to set alpha to 0 for a ridge model (1 for lasso and between these for elastic net)
ridge_tune_grid <- expand.grid(
  "alpha" = c(0),
  "lambda" = seq(0.01, 0.7, by = 0.01)
)

set.seed(2021)
ridge_model <- train(
  logTotalValue ~ .,
  data = data_train,
  method = "glmnet",
  preProcess = c("center", "scale"),
  tuneGrid = ridge_tune_grid,
  trControl = train_control
)

ggplot(ridge_model) + theme_bw()

```

#### LASSO model
Lasso model uses an other type of penalty. It puts a penalty on the sum of the absolute values of coefficients. This also leads to that some coefficients are set exactly to zero, others are only shrunk towards zero. So with lasso we may decrease the number of features more and more as the lambda parameter increases. Here the best lambda was around 0.003. This is really small, meaning we don't exclude a lot of variables.
```{r, echo=F, message=F, warning=F, fig.align='center', cache=F}

#######################
##    LASSO model    ##
#######################
lasso_tune_grid <- expand.grid(
  "alpha" = c(1),
  "lambda" = seq(0.00001, 0.1, by = 0.001) 
)

set.seed(2021)
lasso_model <- train(
  logTotalValue ~ .,
  data = data_train,
  method = "glmnet",
  preProcess = c("center", "scale"),
  tuneGrid = lasso_tune_grid,
  trControl = train_control
)


ggplot(lasso_model) +theme_bw()


```

#### Elastic net
With elastic net we can combine the power of a ridge and a lasso model. So we can simultaneously shrink coefficients towards zero with ridge and perform principled variable selection with lasso. To find the best "mixture" of these I will use different alphas between 0 and 1. The optimal alpha is 0.3 and the optimal lambda is  0.0101 for the elastic net. This means we will use a combination of 30% lasso model and 70% ridge.
```{r, echo=F, message=F, warning=F, fig.align='center', cache=T}
#######################
##    Elastic net    ##
#######################
enet_tune_grid <- expand.grid(
  "alpha" = seq(0, 1, by = 0.1),
  "lambda" = seq(0.0001, 0.2, by = 0.01)
)

set.seed(2021)
enet_model <- train(
  logTotalValue ~ .,
  data = data_train,
  method = "glmnet",
  preProcess = c("center", "scale"),
  tuneGrid = enet_tune_grid,
  trControl = train_control
)


ggplot(enet_model) + theme_bw()

```

 
```{r, include=F}
## Summary
summary_1 <- resamples(
  list("linear" = linear_model,
       "ridge" = ridge_model,
       "lasso" = lasso_model,
       "elastic net" = enet_model
  )
) 

summary(summary_1)

bwplot(summary_1)
```

#### Summary
To compare our models we will look at RMSE for them. They are really close to each other, but the best is for the Lasso model. 

```{r, echo=F, message=F,warning=F}
model_names <- c("linear_model","ridge_model","lasso_model","enet_model")
rmse_CV <- c()

for (i in model_names) {
  rmse_CV[i]  <- min(get(i)$results$RMSE)
}
rmse_df <- data.frame(rmse_CV)
colnames(rmse_df) <- c("CV RMSE")
rownames(rmse_df) <- c("Linear", "Ridge", "Lasso", "Elastic Net")

rmse_df %>% 
  kbl() %>% 
  kable_classic(full_width = F, html_font = "Cambria")

```



### e) Simplest model that is still good enough
In case we want to look at the simplest but still good models we have to include **selectionFunction = "oneSE"** to the train control in the caret train module. This will look at the highest lambda value which contains the optimal lambda's error within one standard deviation. This will lead to a simpler model meaning smaller coefficient for ridge and less features in the model for LASSO. For elastic net both of these are true.
```{r, echo=F, message=F, warning=F, cache=T}
train_control_1se <- trainControl(method = "cv",number = 10,
                                  verboseIter = FALSE,
                                  selectionFunction = "oneSE")  


#######################
##    Ridge model    ##
#######################
set.seed(2021)
ridge_model_1se <- train(
  logTotalValue ~ .,
  data = data_train,
  method = "glmnet",
  preProcess = c("center", "scale"),
  tuneGrid = ridge_tune_grid,
  trControl = train_control_1se
)


#######################
##    LASSO model    ##
#######################
set.seed(2021)
lasso_model_1se <- train(
  logTotalValue ~ .,
  data = data_train,
  method = "glmnet",
  preProcess = c("center", "scale"),
  tuneGrid = lasso_tune_grid,
  trControl = train_control_1se
)

#######################
##    Elastic net    ##
#######################
set.seed(2021)
enet_model_1se <- train(
  logTotalValue ~ .,
  data = data_train,
  method = "glmnet",
  preProcess = c("center", "scale"),
  tuneGrid = enet_tune_grid,
  trControl = train_control_1se
)

## Summary
summary_2 <- resamples(
  list("Linear" = linear_model,
       "RIDGE" = ridge_model,
       "LASSO" = lasso_model,
       "Elastic Net" = enet_model,
       "RIDGE_1se" = ridge_model_1se,
       "LASSO_1se" = lasso_model_1se,
       "Elastic_Net_1se" = enet_model_1se)) 

summary(summary_2)

```

#### Summary
We can see that based on RMSE values looking at the simplest models increase a bit, while R squared decreases.



### f) Using PCA for dimensionality reduction 
After looking at penalized models lets do PCA. I checked a simple PCA models with tune grid 1:100 to make sure that the high number of factor variables are took into consideration. 27 components capture already 99.9% of the variance, while 28 captures 99.99%. The optimal number in the end was 97.

```{r, echo=F, message=F, warning=F, cache=T}

preProcess(data, method = c("center", "scale", "pca"), thresh = 0.999)
preProcess(data, method = c("center", "scale", "pca"), thresh = 0.999999)


set.seed(2021)
pcr_model <- train(
  logTotalValue ~ . ,
  data = data,
  method = "pcr",
  trControl = trainControl(method = "cv", number = 10),
  tuneGrid = data.frame(ncomp = 1:100),
  preProcess = c("center", "scale")
)

pcr_model

```

### g) Apply PCA prior to estimating penalized models
To apply PCA I needed to put is to preProcess and to trConrol. For train control I chose a thresh so PCA will cover 99% of variation. 

```{r, echo=F, message=F, warning=F, cache=T}
train_control_PCA <- trainControl(method = "cv", 
                             number = 10, 
                             preProcOptions = list(thresh=0.99)
)

#######################
##    Ridge model    ##
#######################
set.seed(2021)
ridge_model_pca <- train(
  logTotalValue ~ .,
  data = data_train,
  method = "glmnet",
  preProcess = c("center", "scale", "pca", "nzv"),
  tuneGrid = ridge_tune_grid,
  trControl = train_control_PCA
)


#######################
##    LASSO model    ##
#######################
set.seed(2021)
lasso_model_pca <- train(
  logTotalValue ~ .,
  data = data_train,
  method = "glmnet",
  preProcess = c("center", "scale", "pca", "nzv"),
  tuneGrid = lasso_tune_grid,
  trControl = train_control_PCA
)

#######################
##    Elastic net    ##
#######################
set.seed(2021)
enet_model_pca <- train(
  logTotalValue ~ .,
  data = data_train,
  method = "glmnet",
  preProcess = c("center", "scale", "pca", "nzv"),
  tuneGrid = enet_tune_grid,
  trControl = train_control_PCA
)


## Summary
summary_3 <- resamples(
  list("Linear" = linear_model,
       "RIDGE" = ridge_model,
       "LASSO" = lasso_model,
       "Elastic Net" = enet_model,
       "RIDGE_1se" = ridge_model_1se,
       "LASSO_1se" = lasso_model_1se,
       "Elastic_Net_1se" = enet_model_1se,
       "RIDGE_pca" = ridge_model_pca,
       "LASSO_pca" = lasso_model_pca,
       "Elastic_Net_pca" = enet_model_pca)) 

summary(summary_3)

bwplot(summary_3)

```


### h) Select the best model 
We looked at the rmse values again. There are no significant differences, but the best model is the lasso based on the smallest RMSE and the largest R squared.  Based on this I will use the lasso as the final model.
I evaluate my best model by looking at RMSE value in the test set. In this case it is 0.68. 
```{r, include=F}
model_names <- c("linear_model","ridge_model","lasso_model","enet_model", "ridge_model_1se","lasso_model_1se","enet_model_1se", "ridge_model_pca","lasso_model_pca","enet_model_pca")
rmse_CV3 <- c()
for (i in model_names) {
  rmse_CV3[i]  <- min(get(i)$results$RMSE)
}
rmse_df3 <- data.frame(rmse_CV3)
colnames(rmse_df3) <- "CV RMSE"
rownames(rmse_df3) <- c("Linear", "Ridge", "Lasso", "Elastic Net", "Simplest Ridge", "Simplest Lasso", "Simplest Elastic Net", "Ridge with PCA", "Lasso with PCA", "Elastic net with PCA")

rmse_df3 %>% 
  kbl() %>% 
  kable_classic(full_width = F, html_font = "Cambria")

# This gives me different RMSEs same for 1se and basics so I assume there is an error in this
```


```{r, echo=F, message=F, warning=F}
model_differences <- diff(summary_3)

summary(model_differences)

dotplot(model_differences)



data_test$best_pred <- predict(lasso_model, newdata = data_test)

best_model_RMSE <- RMSE(data_test$best_pred, data_test$logTotalValue)

best_model_RMSE
```


## 2. Clustering on the USArrests dataset

In this exercise I will use the USAArrests data set. I will apply clustering then make sense of the clusters using the principal components.

```{r, include=F}
df <- USArrests
skim(df)
```

```{r, echo=F, message=F, warning=F}
ggpairs(df, columns = c("Murder", "Assault", "UrbanPop","Rape"))
```


### a) Data pre-processing
In this data set we only have 4 variables. There are no missing values and no outliers. Their distributions are not sharply skewed. Based on these we don't need any transformations. For PCA analysis it is important to center and scale the data. For clustering we can think of scaling the data so the distances are more similar. Here I think the values are not that different so I won't do any preprocessing steps.


### b) Determine the optimal number of clusters
We can use this so called elbow graph to see the visual representation of how the change is cluster number effect the total within sum of squares values.
With the NbClust formula I calculate the optimal number by majority voting. In my case 9 proposed 2 as the best number of clusters and 6 proposed 6. So based on majority voting 2 is selected as the final best.
```{r, echo=F, message=F, warning=F}
fviz_nbclust(df, kmeans, method = "wss")

nb <- NbClust(df, method = "kmeans", min.nc = 2, max.nc = 13, index = "all")
nb
```

### c) Use the k-means method to cluster states
I will use 2 clusters as calculated above and 20 different starting points as it can change the outcome. From the graphs you can see that clusters are nicely separatable. The most devided graph is for
```{r, echo=F, message=F, warning=F, out.width="33%"}
set.seed(2021)
km <- kmeans(df, centers = 2, nstart = 20)
km

# print(km$centers)
# print(table(km$cluster))
# print(km$withinss)


data_w_clusters <- mutate(df, cluster = factor(km$cluster))

ggplot(data_w_clusters, aes(x = UrbanPop, y = Murder, color = cluster)) +
  geom_point() +
  theme_bw()

ggplot(data_w_clusters, aes(x = UrbanPop, y = Assault, color = cluster)) +
  geom_point() +
  theme_bw()

ggplot(data_w_clusters, aes(x = UrbanPop, y = Rape, color = cluster)) +
  geom_point() +
  theme_bw()

```

### d) Perform PCA and get the first two principal component coordinates for all observations 
I got the first two principal component. Here I used scaling.
```{r,  message=F, warning=F}

pca_result <- prcomp(df, scale = TRUE)
first_two_pc <- as_tibble(pca_result$x[, 1:2])
```

### e) Plot clusters in the coordinate system defined by the first two principal components
It is really nicely visible that clusters are separate on the graph. It can be also seen that the first PCA really separates the clusters. 
```{r, echo=F, message=F, warning=F}
pc_data_w_clusters <- mutate(first_two_pc, cluster = factor(km$cluster))

ggplot(pc_data_w_clusters, aes(x = PC1, y = PC2, color = cluster)) +
  geom_point() +
  theme_bw()
```



# 3. PCA of high-dimensional data
In the last exercise I will perform PCA on 40 observations of 1000 variables. For this I will use data about genes of tissues of healthy and diseased patients.
```{r, include=F}
genes <- read_csv("https://www.statlearning.com/s/Ch10Ex11.csv", col_names = FALSE) %>%
  t() %>% as_tibble()  # the original dataset is of dimension 1000x40 so we transpose it
dim(data)
```

### a) Perform PCA on this data with scaling features
I centered and scaled the features and then preformed a PCA. Out of the 1000 features 39 components are already covering 99.9% of the varience. We learned that we have the same amount of PCAs as the number of variables. After thinking it through I would say in this case it is not 100% true. As we have only 40 observations they cannot create more than 39 dimensions so there is no sense in looking at more component than 39. (Think of 3 observations and 10 features. Even though theoretically we can calculate 10 PCAs three dots can only determine 2 dimensions.)
```{r, echo=F, message=F, warning=F}
preProcess(genes, method = c("center", "scale", "pca"), thresh = 0.999)

pca_genes <- prcomp(genes, center = TRUE, scale=TRUE)
plot(pca_genes, type='l')
#summary(pca_genes)
```

### b) Visualize data points in the space of the first two principal components
There are two  significant categories seen on this graph. PCA1 can devide the observations to two separate categories quite nicely. And is doues it perfectly look that the pationts from 1 to 20 are in one category while from 21 to 40 are in another.
```{r, echo=F, message=F, warning=F}
fviz_pca_ind(pca_genes, 
             axes = c(1, 2),
             geom = c("point", "text"),
             col.ind = "black",
             fill.ind = "white",
             col.ind.sup = "blue")

#fviz_pca(pca_genes, scale = 0)

```

### c) Which individual features can matter
With this calculation I check what features matter the most for PCA1 as it is created as the linear combination of variables it can be seen what variables have to most effect in the combination. For this we need to check the largest coordinates in absolute terms. The two  features that matter the most are V502 and V589. We can do one-by-one plot to see how the points relate. We can see that these two features already determine a linear pattern in the data. These two variables are positively correlated.
```{r,  message=F, warning=F}
tdf <- data.frame(pca_genes$rotation)

ordered <- tdf %>% 
  arrange(-abs(tdf$PC1))


ggplot(genes, aes(x = V502, y = V589)) +
  geom_point()
```

PCA thus offers a way to summarize vast amounts of variables in a handful of dimensions. This can serve as a tool to pick interesting variables where, for example, visual inspection would be hopeless















